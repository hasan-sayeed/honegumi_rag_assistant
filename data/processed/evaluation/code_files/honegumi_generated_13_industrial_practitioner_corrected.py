# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


obj1_name = "branin"


def branin(x1, x2):
    y = float(
        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)
        + 10
    )

    return y


ax_client = AxClient()

ax_client.create_experiment(
    parameters=[
        {"name": "x1", "type": "range", "bounds": [-5.0, 10.0]},
        {"name": "x2", "type": "range", "bounds": [0.0, 10.0]},
    ],
    objectives={
        obj1_name: ObjectiveProperties(minimize=True),
    },
)


for i in range(19):

    parameterization, trial_index = ax_client.get_next_trial()

    # extract parameters
    x1 = parameterization["x1"]
    x2 = parameterization["x2"]

    results = branin(x1, x2)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)

best_parameters, metrics = ax_client.get_best_parameters()


# Plot results
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
ax.scatter(df.index, df[objectives], ec="k", fc="none", label="Observed")
ax.plot(
    df.index,
    np.minimum.accumulate(df[objectives]),
    color="#0033FF",
    lw=2,
    label="Best to Trial",
)
ax.set_xlabel("Trial Number")
ax.set_ylabel(objectives[0])

ax.legend()
plt.show()

# Branin benchmark: compare EI, UCB, and Thompson Sampling on Ax
# %pip install ax-platform==0.4.3 matplotlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch

from ax.service.ax_client import AxClient, ObjectiveProperties
from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
from ax.modelbridge.registry import Models

from botorch.acquisition.monte_carlo import (
    qNoisyExpectedImprovement,
    qUpperConfidenceBound,
    qThompsonSampling,
)


# ----------------------------
# Branin function (minimize)
# Domain: x in [-5, 10], y in [0, 15]
# Known global minimum value for this formulation: ~0.3978873577
# ----------------------------
def branin_value(x: float, y: float) -> float:
    term1 = y - (5.1 / (4.0 * np.pi**2)) * x**2 + (5.0 / np.pi) * x - 6.0
    term2 = 10.0 * (1.0 - 1.0 / (8.0 * np.pi)) * np.cos(x)
    return float(term1**2 + term2 + 10.0)


# ----------------------------
# Configuration
# ----------------------------
PARAM_X_NAME = "x"
PARAM_Y_NAME = "y"
OBJECTIVE_NAME = "branin_value"

BOUNDS = {
    PARAM_X_NAME: (-5.0, 10.0),
    PARAM_Y_NAME: (0.0, 15.0),
}

# Noise: we simulate measurement noise; Ax will infer noise if SEM is omitted
NOISE_SD = 0.1
GLOBAL_MIN = 0.39788735772973816

NUM_REPEATS = 20
NUM_EVALS_TOTAL = 30
NUM_SOBOL_INIT = 5  # number of initial quasi-random evaluations before BO
UCB_BETA = 0.2  # exploration parameter for UCB


# ----------------------------
# Helper: build generation strategy per acquisition
# ----------------------------
def build_generation_strategy(acqf_name: str) -> GenerationStrategy:
    if acqf_name == "EI":
        acqf_class = qNoisyExpectedImprovement
        acquisition_options = {}
    elif acqf_name == "UCB":
        acqf_class = qUpperConfidenceBound
        acquisition_options = {"beta": UCB_BETA}
    elif acqf_name == "TS":
        acqf_class = qThompsonSampling
        acquisition_options = {}
    else:
        raise ValueError(f"Unknown acquisition function: {acqf_name}")

    gs = GenerationStrategy(
        steps=[
            GenerationStep(
                model=Models.SOBOL,
                num_trials=NUM_SOBOL_INIT,
                min_trials_observed=NUM_SOBOL_INIT,
            ),
            GenerationStep(
                model=Models.BOTORCH_MODULAR,
                num_trials=-1,
                model_kwargs={},
                model_gen_kwargs={
                    "botorch_acqf_class": acqf_class,
                    "acquisition_options": acquisition_options,
                },
            ),
        ]
    )
    return gs


# ----------------------------
# Helper: evaluate with noise
# ----------------------------
def evaluate_branin_with_noise(params: dict, rng: np.random.Generator) -> float:
    x = float(params[PARAM_X_NAME])
    y = float(params[PARAM_Y_NAME])
    true_val = branin_value(x, y)
    noisy_val = true_val + rng.normal(0.0, NOISE_SD)
    return float(noisy_val)


# ----------------------------
# Single run of an Ax optimization with specified acquisition function
# Returns the sequence of best-so-far objective values (length NUM_EVALS_TOTAL)
# ----------------------------
def run_single_repeat(acqf_name: str, seed: int) -> np.ndarray:
    torch.manual_seed(seed)
    np.random.seed(seed)
    rng = np.random.default_rng(seed)

    ax_client = AxClient(generation_strategy=build_generation_strategy(acqf_name))

    ax_client.create_experiment(
        name=f"branin_{acqf_name}_seed_{seed}",
        parameters=[
            {
                "name": PARAM_X_NAME,
                "type": "range",
                "bounds": [BOUNDS[PARAM_X_NAME][0], BOUNDS[PARAM_X_NAME][1]],
                "value_type": "float",
            },
            {
                "name": PARAM_Y_NAME,
                "type": "range",
                "bounds": [BOUNDS[PARAM_Y_NAME][0], BOUNDS[PARAM_Y_NAME][1]],
                "value_type": "float",
            },
        ],
        objectives={OBJECTIVE_NAME: ObjectiveProperties(minimize=True)},
    )

    best_so_far = []
    incumbent = np.inf

    for _ in range(NUM_EVALS_TOTAL):
        parameters, trial_index = ax_client.get_next_trial()
        obs = evaluate_branin_with_noise(parameters, rng=rng)
        ax_client.complete_trial(trial_index=trial_index, raw_data=obs)

        incumbent = min(incumbent, obs)
        best_so_far.append(incumbent)

    return np.array(best_so_far, dtype=float)


# ----------------------------
# Run benchmark: compare EI, UCB, Thompson Sampling
# ----------------------------
if __name__ == "__main__":
    methods = ["EI", "UCB", "TS"]
    all_results = {m: np.zeros((NUM_REPEATS, NUM_EVALS_TOTAL), dtype=float) for m in methods}

    print(f"Running Branin benchmark with {NUM_REPEATS} repeats x {NUM_EVALS_TOTAL} evaluations")
    print(f"Acquisitions: {methods}")
    for method in methods:
        print(f"\nMethod: {method}")
        for r in range(NUM_REPEATS):
            seed = 1000 + r  # distinct, reproducible seeds per repeat
            trace = run_single_repeat(acqf_name=method, seed=seed)
            # Convert to best-so-far (monotone nonincreasing), in case noise temporarily improves
            best_trace = np.minimum.accumulate(trace)
            all_results[method][r, :] = best_trace
            if (r + 1) % 5 == 0:
                print(f"  Completed repeat {r + 1}/{NUM_REPEATS}")

    # ----------------------------
    # Aggregate and visualize
    # ----------------------------
    plt.figure(figsize=(7.5, 5.0), dpi=150)
    colors = {"EI": "#0033FF", "UCB": "#CC5500", "TS": "#008B8B"}

    for method in methods:
        traces = all_results[method]  # shape (repeats, evals)
        mean_trace = traces.mean(axis=0)
        std_trace = traces.std(axis=0)
        sem_trace = std_trace / np.sqrt(NUM_REPEATS)

        iters = np.arange(1, NUM_EVALS_TOTAL + 1)
        plt.plot(iters, mean_trace, label=f"{method} (mean)", color=colors[method], lw=2)
        plt.fill_between(
            iters,
            mean_trace - 1.96 * sem_trace,
            mean_trace + 1.96 * sem_trace,
            color=colors[method],
            alpha=0.15,
            linewidth=0,
            label=f"{method} (95% CI)" if method == methods[-1] else None,  # avoid duplicate legend entries
        )

    plt.axhline(GLOBAL_MIN, color="k", ls="--", lw=1, label="Global minimum")
    plt.xlabel("Evaluation number")
    plt.ylabel("Best-so-far objective (branin_value)")
    plt.title(f"Branin minimization with noise sd={NOISE_SD} | {NUM_REPEATS} repeats x {NUM_EVALS_TOTAL} evals")
    plt.legend()
    plt.tight_layout()
    plt.show()

    # ----------------------------
    # Summary table (final best and simple regret)
    # ----------------------------
    summary_rows = []
    for method in methods:
        final_bests = all_results[method][:, -1]
        simple_regret = final_bests - GLOBAL_MIN
        summary_rows.append(
            {
                "method": method,
                "final_best_mean": float(final_bests.mean()),
                "final_best_std": float(final_bests.std(ddof=1)),
                "regret_mean": float(simple_regret.mean()),
                "regret_std": float(simple_regret.std(ddof=1)),
                "median_final_best": float(np.median(final_bests)),
            }
        )

    summary_df = pd.DataFrame(summary_rows).set_index("method").sort_values("final_best_mean")
    print("\nSummary across repeats (lower is better):")
    print(summary_df.round(4))