# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


obj1_name = "branin"
obj2_name = "branin_swapped"


def branin3_moo(x1, x2, x3):
    y = float(
        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)
        + 10
    )

    # Contrived way to incorporate x3 into the objective
    y = y * (1 + 0.1 * x1 * x2 * x3)

    # second objective has x1 and x2 swapped
    y2 = float(
        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)
        + 10
    )

    # Contrived way to incorporate x3 into the second objective
    y2 = y2 * (1 - 0.1 * x1 * x2 * x3)

    return {obj1_name: y, obj2_name: y2}


# Define total for compositional constraint, where x1 + x2 + x3 == total
total = 10.0


# Define the training data

# note that for this training data, the compositional constraint is satisfied


X_train = pd.DataFrame(
    [
        {"x1": 4.0, "x2": 5.0, "x3": 1.0},
        {"x1": 0.0, "x2": 6.2, "x3": 3.8},
        {"x1": 5.9, "x2": 2.0, "x3": 2.0},
        {"x1": 1.5, "x2": 2.0, "x3": 6.5},
        {"x1": 1.0, "x2": 9.0, "x3": 0.0},
    ]
)

# Define y_train (normally the values would be supplied directly instead of calculating here)
y_train = [
    branin3_moo(row["x1"], row["x2"], row["x3"]) for _, row in X_train.iterrows()
]

# See https://youtu.be/4tnaL9ts6CQ for simple human-in-the-loop BO instructions

# Define the number of training examples
n_train = len(X_train)


ax_client = AxClient()

ax_client.create_experiment(
    parameters=[
        {"name": "x1", "type": "range", "bounds": [0.0, total]},
        {"name": "x2", "type": "range", "bounds": [0.0, total]},
    ],
    objectives={
        obj1_name: ObjectiveProperties(minimize=True, threshold=25.0),
        obj2_name: ObjectiveProperties(minimize=True, threshold=15.0),
    },
    parameter_constraints=[
        f"x1 + x2 <= {total}",  # reparameterized compositional constraint, which is a type of sum constraint
    ],
)

# Add existing data to the AxClient
for i in range(n_train):
    parameterization = X_train.iloc[i].to_dict()

    # remove x3, since it's hidden from search space due to composition constraint
    parameterization.pop("x3")

    ax_client.attach_trial(parameterization)
    ax_client.complete_trial(trial_index=i, raw_data=y_train[i])


for i in range(21):

    parameterization, trial_index = ax_client.get_next_trial()

    # extract parameters
    x1 = parameterization["x1"]
    x2 = parameterization["x2"]
    x3 = total - (x1 + x2)  # composition constraint: x1 + x2 + x3 == total

    results = branin3_moo(x1, x2, x3)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)
pareto_results = ax_client.get_pareto_optimal_parameters()


# Plot results
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
pareto = ax_client.get_pareto_optimal_parameters(use_model_predictions=False)
pareto_data = [p[1][0] for p in pareto.values()]
pareto = pd.DataFrame(pareto_data).sort_values(objectives[0])

ax.scatter(df[objectives[0]], df[objectives[1]], fc="None", ec="k", label="Observed")
ax.plot(
    pareto[objectives[0]],
    pareto[objectives[1]],
    color="#0033FF",
    lw=2,
    label="Pareto Front",
)
ax.set_xlabel(objectives[0])
ax.set_ylabel(objectives[1])

ax.legend()
plt.show()

# Generated from Honegumi skeleton and adapted for polymer design MOO with Ax
# %pip install ax-platform==0.4.3 matplotlib numpy pandas
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


# Seed for reproducibility of the simulator and Ax's initial choices
np.random.seed(42)


# Metric names (objectives)
STRENGTH_METRIC = "strength_MPa"
BIODEG_METRIC = "biodegradability_score"


def evaluate_polymer(
    monomer_1: float,
    monomer_2: float,
    monomer_3: float,
    monomer_4: float,
    monomer_5: float,
    extrusion_rate_mm_per_s: float,
    temperature_celsius: float,
) -> dict:
    """
    Simulated evaluation of a polymer formulation and processing.

    Returns:
      - strength_MPa: Larger is better, target threshold of >= 70 MPa.
      - biodegradability_score: Larger is better (0..1 typical).
    Notes:
      This is a surrogate simulator. Replace with actual lab/simulation measurement.
    """

    # Normalize temperature to [0, 1] over [120, 200] C for smoother effects
    t_norm = (temperature_celsius - 120.0) / (200.0 - 120.0)
    t_norm = np.clip(t_norm, 0.0, 1.0)

    # Normalize extrusion rate to [0, 1] over [0.01, 0.1] mm/s
    r_norm = (extrusion_rate_mm_per_s - 0.01) / (0.1 - 0.01)
    r_norm = np.clip(r_norm, 0.0, 1.0)

    # Hypothetical structure-property heuristics:
    # - Strength increases with higher fractions of monomer_1 and monomer_3, synergistic term,
    #   and moderately with higher temperature. Too high extrusion rate slightly penalizes strength.
    # - Biodegradability increases with monomer_2 and monomer_5, and decreases with higher temperature
    #   and with formulations that strongly maximize strength.
    #
    # Base strengths for monomers (arbitrary but plausible)
    base_strength = (
        85.0 * monomer_1
        + 70.0 * monomer_3
        + 60.0 * monomer_4
        + 55.0 * monomer_2
        + 50.0 * monomer_5
    )
    synergy_strength = 30.0 * (monomer_1 * monomer_3)  # synergy term
    temp_effect = 10.0 * t_norm  # hotter improves strength
    rate_penalty = -5.0 * (r_norm**1.2)  # faster extrusion slightly reduces

    strength = base_strength + synergy_strength + temp_effect + rate_penalty

    # Biodegradability components
    base_biodeg = (
        0.7 * monomer_2
        + 0.6 * monomer_5
        + 0.4 * monomer_4
        + 0.2 * monomer_3
        + 0.1 * monomer_1
    )
    # Penalize high strength mixes (trade-off) and high T, fast rate
    strength_tradeoff = -0.002 * max(0.0, strength - 60.0)
    temp_tradeoff = -0.2 * t_norm
    rate_tradeoff = -0.05 * r_norm

    biodeg = base_biodeg + strength_tradeoff + temp_tradeoff + rate_tradeoff

    # Clamp biodegradability to [0, 1] and add small noise
    biodeg = float(np.clip(biodeg, 0.0, 1.0))
    # Add measurement noise (heteroskedastic-ish)
    strength_noise = np.random.normal(loc=0.0, scale=1.0 + 0.5 * t_norm)
    biodeg_noise = np.random.normal(loc=0.0, scale=0.02 + 0.01 * r_norm)

    strength_measured = float(strength + strength_noise)
    biodeg_measured = float(np.clip(biodeg + biodeg_noise, 0.0, 1.0))

    return {
        STRENGTH_METRIC: strength_measured,
        BIODEG_METRIC: biodeg_measured,
    }


# Reparameterized compositional constraint:
# We expose only monomer_1..monomer_4 to Ax; enforce sum <= 1.0
# Then compute monomer_5 = 1.0 - (m1 + m2 + m3 + m4) at evaluation time.
TOTAL_FRACTION = 1.0

# Define 3 historical data points (example: literature values or prior experiments)
# Replace these with your actual historical parameterizations if available.
historical_params = pd.DataFrame(
    [
        {
            "monomer_1": 0.30,
            "monomer_2": 0.25,
            "monomer_3": 0.20,
            "monomer_4": 0.15,
            # monomer_5 is implied by sum-to-1 constraint
            "extrusion_rate": 0.05,  # mm/s
            "temperature": 170.0,  # C
        },
        {
            "monomer_1": 0.15,
            "monomer_2": 0.35,
            "monomer_3": 0.10,
            "monomer_4": 0.20,
            "extrusion_rate": 0.03,
            "temperature": 150.0,
        },
        {
            "monomer_1": 0.25,
            "monomer_2": 0.10,
            "monomer_3": 0.30,
            "monomer_4": 0.20,
            "extrusion_rate": 0.08,
            "temperature": 190.0,
        },
    ]
)

# Compute outcomes for the 3 historical points using the simulator
historical_outcomes = []
for _, row in historical_params.iterrows():
    m1 = float(row["monomer_1"])
    m2 = float(row["monomer_2"])
    m3 = float(row["monomer_3"])
    m4 = float(row["monomer_4"])
    m5 = float(TOTAL_FRACTION - (m1 + m2 + m3 + m4))
    # Ensure numerical safety if replacing with your own data
    m5 = float(np.clip(m5, 0.0, 1.0))
    outcomes = evaluate_polymer(
        m1,
        m2,
        m3,
        m4,
        m5,
        extrusion_rate_mm_per_s=float(row["extrusion_rate"]),
        temperature_celsius=float(row["temperature"]),
    )
    historical_outcomes.append(outcomes)

n_train = len(historical_params)

# Create Ax client and experiment
ax_client = AxClient(random_seed=42)

# Objective thresholds:
# - Strength: we want >= 70 MPa, so set threshold=70.0 (maximize)
# - Biodegradability: if unknown, set a reasonable baseline (e.g., 0.5 on [0,1])
ax_client.create_experiment(
    name="polymer_design_moo",
    parameters=[
        {"name": "monomer_1", "type": "range", "bounds": [0.0, 1.0]},
        {"name": "monomer_2", "type": "range", "bounds": [0.0, 1.0]},
        {"name": "monomer_3", "type": "range", "bounds": [0.0, 1.0]},
        {"name": "monomer_4", "type": "range", "bounds": [0.0, 1.0]},
        {"name": "extrusion_rate", "type": "range", "bounds": [0.01, 0.1]},  # mm/s
        {"name": "temperature", "type": "range", "bounds": [120.0, 200.0]},  # C
    ],
    objectives={
        STRENGTH_METRIC: ObjectiveProperties(minimize=False, threshold=70.0),
        BIODEG_METRIC: ObjectiveProperties(minimize=False, threshold=0.5),
    },
    parameter_constraints=[
        # Compositional constraint reparameterized: expose 4 monomers and enforce sum <= 1.0.
        "monomer_1 + monomer_2 + monomer_3 + monomer_4 <= 1.0",
    ],
    overwrite_existing_experiment=True,
)

# Attach existing data to the AxClient
for i in range(n_train):
    parameterization = historical_params.iloc[i].to_dict()
    # Only monomers 1..4 are in the search space; monomer_5 is implicit
    # So parameterization already fits the search space (monomer_5 is not included)
    ax_client.attach_trial(parameterization)
    ax_client.complete_trial(trial_index=i, raw_data=historical_outcomes[i])

# Total budget of 35 trials (including 3 attached = 32 new ones)
total_budget = 35
n_new_trials = max(0, total_budget - n_train)

for _ in range(n_new_trials):
    parameterization, trial_index = ax_client.get_next_trial()

    # Extract parameters from Ax suggestion
    m1 = float(parameterization["monomer_1"])
    m2 = float(parameterization["monomer_2"])
    m3 = float(parameterization["monomer_3"])
    m4 = float(parameterization["monomer_4"])
    # Compute monomer_5 via the compositional constraint
    m5 = float(TOTAL_FRACTION - (m1 + m2 + m3 + m4))
    # Numerical safety: if floating error produces tiny negatives or >1, clip
    m5 = float(np.clip(m5, 0.0, 1.0))

    extrusion_rate = float(parameterization["extrusion_rate"])
    temperature = float(parameterization["temperature"])

    results = evaluate_polymer(
        m1,
        m2,
        m3,
        m4,
        m5,
        extrusion_rate_mm_per_s=extrusion_rate,
        temperature_celsius=temperature,
    )
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)

# Retrieve Pareto-optimal parameters and plot the observed Pareto front
pareto_results = ax_client.get_pareto_optimal_parameters(use_model_predictions=False)

# Build a dataframe of trial results for plotting
df = ax_client.get_trials_data_frame()
objectives = ax_client.objective_names

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
ax.scatter(df[objectives[0]], df[objectives[1]], fc="None", ec="k", label="Observed")

# Extract Pareto points for plotting
pareto_data = [p[1][0] for p in pareto_results.values()]
pareto_df = pd.DataFrame(pareto_data).sort_values(objectives[0])

ax.plot(
    pareto_df[objectives[0]],
    pareto_df[objectives[1]],
    color="#0033FF",
    lw=2,
    label="Pareto Front",
)
ax.set_xlabel(objectives[0])
ax.set_ylabel(objectives[1])
ax.set_title("Polymer Design: Strength vs. Biodegradability")
ax.legend()
plt.tight_layout()
plt.show()