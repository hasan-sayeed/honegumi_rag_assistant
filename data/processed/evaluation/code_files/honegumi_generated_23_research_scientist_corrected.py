# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


obj1_name = "branin"


def branin(x1, x2):
    y = float(
        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)
        + 10
    )

    return y


ax_client = AxClient()

ax_client.create_experiment(
    parameters=[
        {"name": "x1", "type": "range", "bounds": [-5.0, 10.0]},
        {"name": "x2", "type": "range", "bounds": [0.0, 10.0]},
    ],
    objectives={
        obj1_name: ObjectiveProperties(minimize=True),
    },
)


for i in range(19):

    parameterization, trial_index = ax_client.get_next_trial()

    # extract parameters
    x1 = parameterization["x1"]
    x2 = parameterization["x2"]

    results = branin(x1, x2)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)

best_parameters, metrics = ax_client.get_best_parameters()


# Plot results
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
ax.scatter(df.index, df[objectives], ec="k", fc="none", label="Observed")
ax.plot(
    df.index,
    np.minimum.accumulate(df[objectives]),
    color="#0033FF",
    lw=2,
    label="Best to Trial",
)
ax.set_xlabel("Trial Number")
ax.set_ylabel(objectives[0])

ax.legend()
plt.show()

# Generated by adapting a Honegumi skeleton to a neural network hyperparameter optimization problem
# %pip install ax-platform==0.4.3 matplotlib numpy pandas
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


# Domain-specific metric name
metric_name = "validation_accuracy"


def evaluate_neural_network_training(learning_rate: float, batch_size: int, dropout: float) -> float:
    """
    Surrogate evaluation of a neural network's validation accuracy (%).

    This function simulates how validation accuracy might behave as a function of
    learning rate, batch size, and dropout. It includes realistic nonconvex structure,
    interactions, and observation noise to emulate a noisy training process.

    In production:
    - Replace the body with actual training and evaluation code.
    - Return the observed validation accuracy (%) on a held-out validation set.

    Parameters
    ----------
    learning_rate : float
        Optimizer learning rate, expected in [0.0001, 0.01].
    batch_size : int
        Mini-batch size, expected in [16, 128], integers only.
    dropout : float
        Dropout probability, expected in [0.1, 0.5].

    Returns
    -------
    float
        Simulated validation accuracy in percent (%).
    """
    # Clamp inputs to valid ranges (safety measure)
    lr = float(np.clip(learning_rate, 1e-4, 1e-2))
    bs = int(np.clip(int(round(batch_size)), 16, 128))
    dp = float(np.clip(dropout, 0.1, 0.5))

    # Hypothesized optimal region (typical for many CNN/MLP setups)
    lr_opt = 3e-3
    bs_opt = 64
    dp_opt = 0.28

    # Base maximum achievable accuracy in this simulated setting
    base_max = 92.0  # percent

    # Penalties for deviating from optimal values (shape and scale chosen for realism)
    # Learning rate on log10 scale (too small or too large hurts)
    lr_dev = (np.log10(lr) - np.log10(lr_opt)) ** 2
    penalty_lr = 18.0 * lr_dev

    # Batch size on log2 scale (too small or too large hurts training dynamics)
    bs_dev = (np.log2(bs) - np.log2(bs_opt)) ** 2
    penalty_bs = 1.2 * bs_dev

    # Dropout penalty (quadratic around optimal)
    dp_dev = (dp - dp_opt) / 0.10
    penalty_dp = 3.0 * (dp_dev ** 2)

    # Interactions:
    # - High dropout with small batches tends to underfit
    inter_small_bs = max(0.0, (np.log2(bs_opt) - np.log2(bs)))
    penalty_inter1 = 0.8 * (dp / 0.5) * inter_small_bs

    # - Overly large LR with high dropout destabilizes training
    penalty_inter2 = 30.0 * max(0.0, (lr - lr_opt)) * (dp - 0.2)

    # Mild nonconvex ripple with batch size (e.g., hardware/numerical oddities)
    ripple = 0.4 * np.sin(0.5 * np.log2(bs))

    # Add observation noise to reflect stochastic training variability
    rng = np.random.default_rng()
    noise = rng.normal(loc=0.0, scale=0.4)

    accuracy = base_max - (penalty_lr + penalty_bs + penalty_dp + penalty_inter1 + penalty_inter2) + ripple + noise

    # Bound accuracy to a plausible range
    accuracy = float(np.clip(accuracy, 60.0, 95.0))
    return accuracy


# Initialize Ax client
ax_client = AxClient()

# Create experiment matching the problem specification
ax_client.create_experiment(
    name="nn_hyperparameter_optimization",
    parameters=[
        {
            "name": "learning_rate",
            "type": "range",
            "bounds": [0.0001, 0.01],
            "value_type": "float",
            "log_scale": True,  # Learning rates are best explored on log scale
        },
        {
            "name": "batch_size",
            "type": "range",
            "bounds": [16.0, 128.0],
            "value_type": "int",
        },
        {
            "name": "dropout",
            "type": "range",
            "bounds": [0.1, 0.5],
            "value_type": "float",
        },
    ],
    objectives={
        metric_name: ObjectiveProperties(minimize=False),
    },
)

# Optimization budget: 40 trials
n_trials = 40

for i in range(n_trials):
    parameterization, trial_index = ax_client.get_next_trial()

    # Extract and cast parameters
    lr = float(parameterization["learning_rate"])
    bs = int(round(parameterization["batch_size"]))
    dp = float(parameterization["dropout"])

    # Evaluate the model (simulated here)
    try:
        val_acc_percent = evaluate_neural_network_training(lr, bs, dp)
        # For a single noisy objective, Ax allows passing a float directly
        ax_client.complete_trial(trial_index=trial_index, raw_data=val_acc_percent)
    except Exception as e:
        # Mark as failed in case of evaluation error
        ax_client.log_trial_failure(trial_index=trial_index)
        print(f"Trial {trial_index} failed with error: {e}")

# Retrieve best observed parameters and corresponding performance
best_parameters, _ = ax_client.get_best_parameters()
df = ax_client.get_trials_data_frame()

# Compute best observed accuracy from completed trials
if metric_name in df.columns and not df[metric_name].dropna().empty:
    best_observed = float(df[metric_name].max())
else:
    best_observed = float("nan")

print("Best hyperparameters (observed):")
for k, v in best_parameters.items():
    print(f"  - {k}: {v}")
print(f"Best observed validation accuracy: {best_observed:.2f}%")

# Plot results
objectives = ax_client.objective_names
objective_name = objectives[0]

# Keep only completed trials with measurements
df_valid = df[[objective_name]].dropna()
trial_indices = df_valid.index.values
y_vals = df_valid[objective_name].values
best_so_far = np.maximum.accumulate(y_vals)

fig, ax = plt.subplots(figsize=(7, 4), dpi=150)
ax.scatter(trial_indices, y_vals, ec="k", fc="none", label="Observed")
ax.plot(trial_indices, best_so_far, color="#0033FF", lw=2, label="Best to Trial")
ax.set_xlabel("Trial Number")
ax.set_ylabel(f"{objective_name} (%)")
ax.set_title("Neural Network Hyperparameter Optimization (Ax)")
ax.legend()
plt.tight_layout()
plt.show()