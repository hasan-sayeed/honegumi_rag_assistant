# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


obj1_name = "branin"
obj2_name = "branin_swapped"


def branin_moo(x1, x2):
    y = float(
        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)
        + 10
    )

    # second objective has x1 and x2 swapped
    y2 = float(
        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)
        + 10
    )

    return {obj1_name: y, obj2_name: y2}


ax_client = AxClient()

ax_client.create_experiment(
    parameters=[
        {"name": "x1", "type": "range", "bounds": [-5.0, 10.0]},
        {"name": "x2", "type": "range", "bounds": [0.0, 10.0]},
    ],
    objectives={
        obj1_name: ObjectiveProperties(minimize=True),
        obj2_name: ObjectiveProperties(minimize=True),
    },
)


for i in range(19):

    parameterization, trial_index = ax_client.get_next_trial()

    # extract parameters
    x1 = parameterization["x1"]
    x2 = parameterization["x2"]

    results = branin_moo(x1, x2)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)
pareto_results = ax_client.get_pareto_optimal_parameters()


# Plot results
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
pareto = ax_client.get_pareto_optimal_parameters(use_model_predictions=False)
pareto_data = [p[1][0] for p in pareto.values()]
pareto = pd.DataFrame(pareto_data).sort_values(objectives[0])

ax.scatter(df[objectives[0]], df[objectives[1]], fc="None", ec="k", label="Observed")
ax.plot(
    pareto[objectives[0]],
    pareto[objectives[1]],
    color="#0033FF",
    lw=2,
    label="Pareto Front",
)
ax.set_xlabel(objectives[0])
ax.set_ylabel(objectives[1])

ax.legend()
plt.show()

# Drug candidate selection optimization with 3 objectives using Ax (qNEHVI under the hood)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from ax.service.ax_client import AxClient
from ax.service.utils.instantiation import ObjectiveProperties


# Objective metric names
OBJ_PKI = "binding_affinity_pKi"    # maximize
OBJ_LOGS = "solubility_logS"        # maximize
OBJ_LD50 = "toxicity_LD50"          # minimize (note: LD50 lower implies higher toxicity; adjust sign per your convention)

# Global RNG for reproducibility of simulated noise
RNG = np.random.default_rng(seed=12345)


def simulate_admet_from_descriptors(parameters: dict) -> dict:
    """
    Simulate ADMET-like metrics from molecular descriptors.
    This is a realistic stub that encodes typical SAR/ADMET trends:
      - pKi improves around moderate LogP (2-3), moderate TPSA, moderate MW, and reasonable HBD/HBA.
      - Solubility (logS) worsens with higher LogP and MW, and improves with polarity (TPSA).
      - LD50 (mg/kg) is a crude toxicity proxy; lower values imply higher toxicity.
        Here it decreases with lipophilicity and HBD/HBA, and with larger MW, while higher TPSA can improve it.
    Noise is added to emulate experimental variability, and SEMs are provided as fixed values.

    Replace this function with your real evaluation pipeline (e.g., docking + in silico ADMET models or wet-lab results).
    """
    MW = float(parameters["MW"])
    LogP = float(parameters["LogP"])
    TPSA = float(parameters["TPSA"])
    HBD = float(parameters["HBD"])
    HBA = float(parameters["HBA"])

    # Binding affinity (pKi): peak around drug-like region
    pKi = (
        8.8
        - 0.0018 * (MW - 340.0) ** 2
        - 0.050 * (LogP - 2.5) ** 2
        - 0.0010 * (TPSA - 75.0) ** 2
        + 0.08 * HBA
        + 0.06 * HBD
    )
    pKi = float(np.clip(pKi, 3.0, 12.0))
    pKi_noise = float(RNG.normal(0.0, 0.2))
    pKi_sem = 0.2

    # Solubility (logS): lower (more negative) is worse, driven by lipophilicity & MW; polarity helps
    logS = (
        1.1
        - 1.05 * LogP
        - 0.0012 * (MW - 300.0)
        + 0.012 * TPSA
        - 0.05 * HBA
        - 0.07 * HBD
    )
    logS = float(np.clip(logS, -8.0, 2.0))
    logS_noise = float(RNG.normal(0.0, 0.2))
    logS_sem = 0.2

    # Toxicity (LD50 mg/kg): lower is more toxic. We model toxicity increasing with lipophilicity,
    # HBD/HBA, and larger MW, while polarity can mitigate.
    ld50 = (
        1500.0
        - 200.0 * LogP
        - 0.5 * (MW - 300.0)
        - 25.0 * HBD
        - 15.0 * HBA
        + 2.5 * TPSA
    )
    ld50 = float(np.clip(ld50, 50.0, 5000.0))
    ld50_noise = float(RNG.normal(0.0, 75.0))
    ld50_sem = 75.0

    return {
        OBJ_PKI: (pKi + pKi_noise, pKi_sem),
        OBJ_LOGS: (logS + logS_noise, logS_sem),
        OBJ_LD50: (ld50 + ld50_noise, ld50_sem),
    }


def pivot_trials_df(df: pd.DataFrame, objective_names: list[str]) -> pd.DataFrame:
    """
    AxClient.get_trials_data_frame() can return different shapes across versions.
    This helper pivots to a wide format with one row per trial, columns per objective mean.
    """
    if {"metric_name", "mean"}.issubset(df.columns):
        # Tidy format -> pivot wider
        wide = (
            df.pivot_table(
                index="trial_index",
                columns="metric_name",
                values="mean",
                aggfunc="last",
            )
            .reset_index()
            .rename_axis(None, axis=1)
        )
        # Keep only objectives if present
        keep_cols = ["trial_index"] + [m for m in objective_names if m in wide.columns]
        return wide[keep_cols]
    else:
        # Already wide with objective columns present
        cols_present = [c for c in objective_names if c in df.columns]
        if "trial_index" in df.columns:
            keep_cols = ["trial_index"] + cols_present
        else:
            keep_cols = cols_present
        return df[keep_cols].copy()


def extract_pareto_metrics(ax_client: AxClient, objective_names: list[str]) -> pd.DataFrame:
    """
    Extract Pareto-optimal observed means for all objectives into a DataFrame.
    """
    pareto = ax_client.get_pareto_optimal_parameters(use_model_predictions=False)
    rows = []
    for _, (_, metrics) in pareto.items():
        def mean_of(v):
            if isinstance(v, (tuple, list)) and len(v) >= 1:
                return float(v[0])
            return float(v)
        row = {name: mean_of(metrics[name]) for name in objective_names if name in metrics}
        rows.append(row)
    return pd.DataFrame(rows)


def plot_results(ax_client: AxClient) -> None:
    """
    Create pairwise scatter plots for the three objectives and overlay Pareto-optimal points.
    """
    objective_names = list(ax_client.objective_names)
    df_all = ax_client.get_trials_data_frame()
    df_wide = pivot_trials_df(df_all, objective_names)

    # Pareto set (observed)
    pareto_df = extract_pareto_metrics(ax_client, objective_names)

    # Prepare plotting
    fig, axes = plt.subplots(1, 3, figsize=(16, 4.5), dpi=150)

    # 1) pKi vs logS (both maximize)
    if all(name in df_wide.columns for name in [OBJ_PKI, OBJ_LOGS]):
        sc = axes[0].scatter(
            df_wide[OBJ_PKI],
            df_wide[OBJ_LOGS],
            c=df_wide[OBJ_LD50] if OBJ_LD50 in df_wide.columns else None,
            cmap="viridis",
            edgecolor="k",
            alpha=0.8,
            label="Observed",
        )
        if not pareto_df.empty and all(col in pareto_df.columns for col in [OBJ_PKI, OBJ_LOGS]):
            axes[0].scatter(
                pareto_df[OBJ_PKI],
                pareto_df[OBJ_LOGS],
                facecolors="none",
                edgecolors="#FF3B3B",
                s=80,
                linewidths=1.8,
                label="Pareto (observed)",
            )
        axes[0].set_xlabel("Binding affinity pKi (higher is better)")
        axes[0].set_ylabel("Solubility logS (higher is better)")
        axes[0].legend(loc="best")
        if OBJ_LD50 in df_wide.columns:
            cbar = fig.colorbar(sc, ax=axes[0])
            cbar.set_label("LD50 mg/kg (lower is better)")

    # 2) pKi vs LD50 (minimize LD50)
    if all(name in df_wide.columns for name in [OBJ_PKI, OBJ_LD50]):
        axes[1].scatter(
            df_wide[OBJ_PKI],
            df_wide[OBJ_LD50],
            edgecolor="k",
            facecolor="None",
            alpha=0.8,
            label="Observed",
        )
        if not pareto_df.empty and all(col in pareto_df.columns for col in [OBJ_PKI, OBJ_LD50]):
            axes[1].scatter(
                pareto_df[OBJ_PKI],
                pareto_df[OBJ_LD50],
                color="#FF3B3B",
                s=40,
                label="Pareto (observed)",
            )
        axes[1].set_xlabel("Binding affinity pKi (higher is better)")
        axes[1].set_ylabel("Toxicity LD50 mg/kg (lower is better)")

    # 3) logS vs LD50
    if all(name in df_wide.columns for name in [OBJ_LOGS, OBJ_LD50]):
        axes[2].scatter(
            df_wide[OBJ_LOGS],
            df_wide[OBJ_LD50],
            edgecolor="k",
            facecolor="None",
            alpha=0.8,
            label="Observed",
        )
        if not pareto_df.empty and all(col in pareto_df.columns for col in [OBJ_LOGS, OBJ_LD50]):
            axes[2].scatter(
                pareto_df[OBJ_LOGS],
                pareto_df[OBJ_LD50],
                color="#FF3B3B",
                s=40,
                label="Pareto (observed)",
            )
        axes[2].set_xlabel("Solubility logS (higher is better)")
        axes[2].set_ylabel("Toxicity LD50 mg/kg (lower is better)")

    plt.tight_layout()
    plt.show()


def main():
    # Initialize Ax client
    ax_client = AxClient()

    # Create experiment for multi-objective optimization (3 objectives)
    ax_client.create_experiment(
        name="drug_candidate_multiobjective_optimization",
        parameters=[
            {"name": "MW", "type": "range", "bounds": [180.0, 500.0]},   # g/mol
            {"name": "LogP", "type": "range", "bounds": [0.0, 5.0]},
            {"name": "TPSA", "type": "range", "bounds": [20.0, 140.0]},  # Å²
            {"name": "HBD", "type": "range", "bounds": [0.0, 5.0]},      # count
            {"name": "HBA", "type": "range", "bounds": [0.0, 10.0]},     # count
        ],
        objectives={
            OBJ_PKI: ObjectiveProperties(minimize=False),
            OBJ_LOGS: ObjectiveProperties(minimize=False),
            OBJ_LD50: ObjectiveProperties(minimize=True),
        },
    )

    # Optimization loop with total budget of 50 evaluations
    N_TRIALS = 50
    for _ in range(N_TRIALS):
        parameterization, trial_index = ax_client.get_next_trial()
        results = simulate_admet_from_descriptors(parameterization)
        ax_client.complete_trial(trial_index=trial_index, raw_data=results)

    # Retrieve Pareto-optimal parameterizations and metrics (observed)
    pareto = ax_client.get_pareto_optimal_parameters(use_model_predictions=False)

    # Print a brief summary of Pareto-optimal solutions
    print(f"Found {len(pareto)} Pareto-optimal candidates (observed):")
    for i, (arm_name, (params, metrics)) in enumerate(pareto.items(), start=1):
        def mean_of(v):
            if isinstance(v, (tuple, list)) and len(v) >= 1:
                return float(v[0])
            return float(v)
        pki = mean_of(metrics.get(OBJ_PKI, np.nan))
        logs = mean_of(metrics.get(OBJ_LOGS, np.nan))
        ld50 = mean_of(metrics.get(OBJ_LD50, np.nan))
        print(f"#{i:02d} Params={params} -> pKi={pki:.3f}, logS={logs:.3f}, LD50={ld50:.1f} mg/kg")

    # Visualization
    plot_results(ax_client)


if __name__ == "__main__":
    main()