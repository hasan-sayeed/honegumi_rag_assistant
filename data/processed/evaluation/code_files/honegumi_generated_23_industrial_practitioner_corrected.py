# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


obj1_name = "branin"


def branin(x1, x2):
    y = float(
        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)
        + 10
    )

    return y


ax_client = AxClient()

ax_client.create_experiment(
    parameters=[
        {"name": "x1", "type": "range", "bounds": [-5.0, 10.0]},
        {"name": "x2", "type": "range", "bounds": [0.0, 10.0]},
    ],
    objectives={
        obj1_name: ObjectiveProperties(minimize=True),
    },
)


for i in range(19):

    parameterization, trial_index = ax_client.get_next_trial()

    # extract parameters
    x1 = parameterization["x1"]
    x2 = parameterization["x2"]

    results = branin(x1, x2)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)

best_parameters, metrics = ax_client.get_best_parameters()


# Plot results
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
ax.scatter(df.index, df[objectives], ec="k", fc="none", label="Observed")
ax.plot(
    df.index,
    np.minimum.accumulate(df[objectives]),
    color="#0033FF",
    lw=2,
    label="Best to Trial",
)
ax.set_xlabel("Trial Number")
ax.set_ylabel(objectives[0])

ax.legend()
plt.show()

# Generated by Honegumi (adapted to ML hyperparameter optimization)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


# Problem: Maximize validation accuracy (%) of a simple classifier by tuning:
# - learning_rate (0.0001 to 0.01)
# - batch_size (16 to 128, integer)
# - dropout (0.1 to 0.5)
# Budget: 40 trials

# Create a fixed synthetic classification dataset (train/validation) to make this script executable anywhere.
def _make_synthetic_dataset(
    n_train: int = 2000,
    n_val: int = 500,
    n_features: int = 30,
    class_sep: float = 1.5,
    seed: int = 12345,
):
    rng = np.random.default_rng(seed)
    # Two Gaussian blobs with separation
    mean0 = np.zeros(n_features)
    mean1 = np.zeros(n_features)
    mean1[: min(5, n_features)] = class_sep  # separate first few features
    cov = np.eye(n_features)

    X0 = rng.multivariate_normal(mean0, cov, size=(n_train + n_val) // 2)
    X1 = rng.multivariate_normal(mean1, cov, size=(n_train + n_val) // 2)
    y0 = np.zeros((X0.shape[0], 1))
    y1 = np.ones((X1.shape[0], 1))

    X = np.vstack([X0, X1])
    y = np.vstack([y0, y1])

    # Shuffle and split
    idx = rng.permutation(X.shape[0])
    X, y = X[idx], y[idx]

    X_train, y_train = X[:n_train], y[:n_train]
    X_val, y_val = X[n_train : n_train + n_val], y[n_train : n_train + n_val]

    # Standardize by train stats
    mean = X_train.mean(axis=0, keepdims=True)
    std = X_train.std(axis=0, keepdims=True) + 1e-8
    X_train = (X_train - mean) / std
    X_val = (X_val - mean) / std

    return X_train.astype(np.float32), y_train.astype(np.float32), X_val.astype(np.float32), y_val.astype(np.float32)


X_train, y_train, X_val, y_val = _make_synthetic_dataset()


def _sigmoid(z: np.ndarray) -> np.ndarray:
    return 1.0 / (1.0 + np.exp(-z))


def _relu(z: np.ndarray) -> np.ndarray:
    return np.maximum(z, 0.0)


def _initialize_model(n_features: int, hidden_dim: int, rng: np.random.Generator):
    limit1 = np.sqrt(6.0 / (n_features + hidden_dim))
    W1 = rng.uniform(-limit1, limit1, size=(n_features, hidden_dim)).astype(np.float32)
    b1 = np.zeros((hidden_dim,), dtype=np.float32)

    limit2 = np.sqrt(6.0 / (hidden_dim + 1))
    W2 = rng.uniform(-limit2, limit2, size=(hidden_dim, 1)).astype(np.float32)
    b2 = np.zeros((1,), dtype=np.float32)
    return W1, b1, W2, b2


def evaluate_training_run(learning_rate: float, batch_size: int, dropout: float, trial_index: int) -> dict:
    """
    Train a tiny 1-hidden-layer NN with dropout on a fixed synthetic dataset and return validation accuracy (%).
    This function simulates a realistic ML evaluation with stochasticity (random init + dropout), aligning with a noisy objective.

    Returns dict: {"validation_accuracy": (mean, sem)} where mean is %, sem is % standard error (binomial).
    """
    # Sanitize inputs
    lr = float(learning_rate)
    bs = int(max(16, min(128, round(batch_size))))
    p_drop = float(max(0.0, min(0.9, dropout)))  # safety clamp (search bounds ensure 0.1-0.5)

    # Reproducible per-trial randomness
    rng = np.random.default_rng(2025 + int(trial_index))

    n_features = X_train.shape[1]
    hidden_dim = 64
    W1, b1, W2, b2 = _initialize_model(n_features, hidden_dim, rng)

    n_train = X_train.shape[0]
    n_batches = max(1, n_train // bs)
    epochs = 12

    for _ in range(epochs):
        # Shuffle each epoch
        indices = rng.permutation(n_train)
        Xs = X_train[indices]
        ys = y_train[indices]

        for i in range(n_batches):
            start = i * bs
            end = min(n_train, (i + 1) * bs)
            Xb = Xs[start:end]
            yb = ys[start:end]

            # Forward
            z1 = Xb @ W1 + b1  # (B, H)
            h = _relu(z1)

            # Inverted dropout on hidden activations during training
            if p_drop > 0.0:
                mask = (rng.random(h.shape, dtype=np.float32) >= p_drop).astype(np.float32)
                h_drop = h * mask / (1.0 - p_drop)
            else:
                h_drop = h

            logits = h_drop @ W2 + b2  # (B, 1)
            yhat = _sigmoid(logits)

            # Binary cross-entropy loss gradient wrt logits using logistic outputs
            # dL/dlogits = (yhat - y) / B
            B = yb.shape[0]
            dlogits = (yhat - yb) / float(B)

            # Gradients
            dW2 = h_drop.T @ dlogits  # (H,1)
            db2 = dlogits.sum(axis=0)  # (1,)

            dh_drop = dlogits @ W2.T  # (B,H)
            if p_drop > 0.0:
                dh = dh_drop * (mask / (1.0 - p_drop))
            else:
                dh = dh_drop

            dz1 = dh * (z1 > 0.0)  # ReLU backprop
            dW1 = Xb.T @ dz1  # (D,H)
            db1 = dz1.sum(axis=0)  # (H,)

            # SGD update
            W2 -= lr * dW2
            b2 -= lr * db2
            W1 -= lr * dW1
            b1 -= lr * db1

    # Evaluation on validation set (no dropout)
    z1_val = X_val @ W1 + b1
    h_val = _relu(z1_val)
    logits_val = h_val @ W2 + b2
    yhat_val = _sigmoid(logits_val)
    y_pred = (yhat_val >= 0.5).astype(np.float32)
    acc = float((y_pred == y_val).mean()) * 100.0  # percentage

    # Binomial standard error in percent
    p = acc / 100.0
    n_val = y_val.shape[0]
    sem = float(np.sqrt(max(1e-12, p * (1.0 - p)) / n_val) * 100.0)

    return {"validation_accuracy": (acc, sem)}


# Configure Ax client and experiment
ax_client = AxClient()

ax_client.create_experiment(
    parameters=[
        {
            "name": "learning_rate",
            "type": "range",
            "bounds": [0.0001, 0.01],
            "log_scale": True,
            "value_type": "float",
        },
        {
            "name": "batch_size",
            "type": "range",
            "bounds": [16, 128],
            "value_type": "int",
        },
        {
            "name": "dropout",
            "type": "range",
            "bounds": [0.1, 0.5],
            "value_type": "float",
        },
    ],
    objectives={
        "validation_accuracy": ObjectiveProperties(minimize=False),
    },
)

# Run optimization for the given budget
num_trials = 40
for i in range(num_trials):
    parameterization, trial_index = ax_client.get_next_trial()

    # Extract parameters
    lr = parameterization["learning_rate"]
    bs = parameterization["batch_size"]
    dr = parameterization["dropout"]

    # Evaluate and report results
    results = evaluate_training_run(lr, bs, dr, trial_index=trial_index)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)

# Retrieve best parameters and metrics
best_parameters, best_metrics = ax_client.get_best_parameters()
print("Best hyperparameters found:")
print(best_parameters)
print("Best observed validation accuracy (%):", best_metrics["validation_accuracy"]["mean"])
print("Estimated SEM (%):", best_metrics["validation_accuracy"].get("sem", None))

# Plot results
objective_name = ax_client.objective_names[0]
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
y = df[objective_name]
ax.scatter(df.index, y, ec="k", fc="none", label="Observed")
ax.plot(df.index, np.maximum.accumulate(y), color="#0033FF", lw=2, label="Best to Trial")
ax.set_xlabel("Trial Number")
ax.set_ylabel("Validation Accuracy (%)")
ax.set_title("Bayesian Optimization of ML Hyperparameters")
ax.legend()
plt.tight_layout()
plt.show()