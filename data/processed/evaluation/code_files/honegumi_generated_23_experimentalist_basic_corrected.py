# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


objective_name = "validation_accuracy"


def simulate_validation_accuracy(learning_rate: float, batch_size: int, dropout: float):
    """
    Simulated evaluation of a neural network's validation accuracy (%)
    as a function of:
      - learning_rate in [1e-4, 1e-2] (log-scaled effect, optimum near ~2e-3)
      - batch_size in [16, 128] (optimum near ~64)
      - dropout in [0.1, 0.5] (optimum near ~0.25)

    Returns a dict with mean and SEM to reflect run-to-run noise.
    Replace this function with actual model training & evaluation for real use.
    """
    # Base accuracy and optima
    base = 82.0
    lr_opt = 2e-3
    bs_opt = 64
    do_opt = 0.25

    # Learning rate penalty (log-scale parabola)
    lr_log = np.log10(np.clip(learning_rate, 1e-6, 1.0))
    lr_log_opt = np.log10(lr_opt)
    lr_penalty = 18.0 * (lr_log - lr_log_opt) ** 2

    # Batch size penalty (parabolic around bs_opt)
    bs_norm = (batch_size - bs_opt) / (128 - 16)
    bs_penalty = 6.0 * (bs_norm**2)

    # Dropout penalty (parabolic around do_opt)
    do_penalty = 10.0 * (dropout - do_opt) ** 2

    # Mild interaction: too high lr + high dropout hurts more
    interaction_penalty = 6.0 * max(0.0, (learning_rate - lr_opt) / lr_opt) * max(
        0.0, (dropout - do_opt) / do_opt
    )

    mean_acc = base - lr_penalty - bs_penalty - do_penalty - interaction_penalty

    # Clip to realistic bounds
    mean_acc = float(np.clip(mean_acc, 50.0, 99.5))

    # Add realistic noise and SEM (since validation accuracy varies across runs)
    noise_sd = 0.6  # percentage points
    noisy_acc = mean_acc + float(np.random.normal(0.0, noise_sd))
    noisy_acc = float(np.clip(noisy_acc, 0.0, 100.0))

    sem = 0.5  # standard error of the mean (reflecting run-to-run variability)
    return {objective_name: (noisy_acc, sem)}


ax_client = AxClient()

ax_client.create_experiment(
    name="neural_net_hyperparameter_optimization",
    parameters=[
        {
            "name": "learning_rate",
            "type": "range",
            "bounds": [1e-4, 1e-2],
            "value_type": "float",
            "log_scale": True,
        },
        {
            "name": "batch_size",
            "type": "range",
            "bounds": [16.0, 128.0],
            "value_type": "int",
        },
        {
            "name": "dropout",
            "type": "range",
            "bounds": [0.1, 0.5],
            "value_type": "float",
        },
    ],
    objectives={
        objective_name: ObjectiveProperties(minimize=False),
    },
)

TOTAL_TRIALS = 40

for _ in range(TOTAL_TRIALS):
    try:
        parameterization, trial_index = ax_client.get_next_trial()

        # Extract and cast parameters
        lr = float(parameterization["learning_rate"])
        bs = int(parameterization["batch_size"])
        do = float(parameterization["dropout"])

        results = simulate_validation_accuracy(lr, bs, do)
        ax_client.complete_trial(trial_index=trial_index, raw_data=results)
    except Exception:
        # In case of any evaluation failure, mark trial as failed to avoid re-suggesting it
        ax_client.log_trial_failure(trial_index=trial_index)

best_parameters, metrics = ax_client.get_best_parameters()
best_acc_mean = metrics[objective_name]["mean"]
best_acc_sem = metrics[objective_name]["sem"]

print("Best hyperparameters found:")
print(best_parameters)
print(f"Best observed {objective_name}: {best_acc_mean:.3f}% Â± {best_acc_sem:.3f}")

# Plot results
objective_names = ax_client.objective_names
df = ax_client.get_trials_data_frame()

# Robust extraction of the optimization trace for the single objective
obj = objective_names[0]
if obj not in df.columns:
    # Fallback: in some versions, data might be long-form; try to pivot
    if {"metric_name", "mean"}.issubset(df.columns):
        wide = df.pivot_table(values="mean", index=df.index, columns="metric_name")
        if obj in wide.columns:
            series = wide[obj]
        else:
            series = pd.Series(dtype=float)
    else:
        series = pd.Series(dtype=float)
else:
    series = df[obj]

series = series.astype(float).dropna()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
ax.scatter(series.index, series.values, ec="k", fc="none", label="Observed")
ax.plot(
    series.index,
    np.maximum.accumulate(series.values),
    color="#0033FF",
    lw=2,
    label="Best to Trial",
)
ax.set_xlabel("Trial Number")
ax.set_ylabel(f"{obj} (%)")
ax.set_title("Neural Network Hyperparameter Optimization")
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()