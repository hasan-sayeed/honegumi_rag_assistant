# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


obj1_name = "branin"
obj2_name = "branin_swapped"


def branin3_moo(x1, x2, x3):
    y = float(
        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)
        + 10
    )

    # Contrived way to incorporate x3 into the objective
    y = y * (1 + 0.1 * x1 * x2 * x3)

    # second objective has x1 and x2 swapped
    y2 = float(
        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)
        + 10
    )

    # Contrived way to incorporate x3 into the second objective
    y2 = y2 * (1 - 0.1 * x1 * x2 * x3)

    return {obj1_name: y, obj2_name: y2}


# Define total for compositional constraint, where x1 + x2 + x3 == total
total = 10.0


# Define the training data

# note that for this training data, the compositional constraint is satisfied


X_train = pd.DataFrame(
    [
        {"x1": 4.0, "x2": 5.0, "x3": 1.0},
        {"x1": 0.0, "x2": 6.2, "x3": 3.8},
        {"x1": 5.9, "x2": 2.0, "x3": 2.0},
        {"x1": 1.5, "x2": 2.0, "x3": 6.5},
        {"x1": 1.0, "x2": 9.0, "x3": 0.0},
    ]
)

# Define y_train (normally the values would be supplied directly instead of calculating here)
y_train = [
    branin3_moo(row["x1"], row["x2"], row["x3"]) for _, row in X_train.iterrows()
]

# See https://youtu.be/4tnaL9ts6CQ for simple human-in-the-loop BO instructions

# Define the number of training examples
n_train = len(X_train)


ax_client = AxClient()

ax_client.create_experiment(
    parameters=[
        {"name": "x1", "type": "range", "bounds": [0.0, total]},
        {"name": "x2", "type": "range", "bounds": [0.0, total]},
    ],
    objectives={
        obj1_name: ObjectiveProperties(minimize=True, threshold=25.0),
        obj2_name: ObjectiveProperties(minimize=True, threshold=15.0),
    },
    parameter_constraints=[
        f"x1 + x2 <= {total}",  # reparameterized compositional constraint, which is a type of sum constraint
    ],
)

# Add existing data to the AxClient
for i in range(n_train):
    parameterization = X_train.iloc[i].to_dict()

    # remove x3, since it's hidden from search space due to composition constraint
    parameterization.pop("x3")

    ax_client.attach_trial(parameterization)
    ax_client.complete_trial(trial_index=i, raw_data=y_train[i])


for i in range(21):

    parameterization, trial_index = ax_client.get_next_trial()

    # extract parameters
    x1 = parameterization["x1"]
    x2 = parameterization["x2"]
    x3 = total - (x1 + x2)  # composition constraint: x1 + x2 + x3 == total

    results = branin3_moo(x1, x2, x3)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)
pareto_results = ax_client.get_pareto_optimal_parameters()


# Plot results
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
pareto = ax_client.get_pareto_optimal_parameters(use_model_predictions=False)
pareto_data = [p[1][0] for p in pareto.values()]
pareto = pd.DataFrame(pareto_data).sort_values(objectives[0])

ax.scatter(df[objectives[0]], df[objectives[1]], fc="None", ec="k", label="Observed")
ax.plot(
    pareto[objectives[0]],
    pareto[objectives[1]],
    color="#0033FF",
    lw=2,
    label="Pareto Front",
)
ax.set_xlabel(objectives[0])
ax.set_ylabel(objectives[1])

ax.legend()
plt.show()

# Generated for polymer formulation multi-objective optimization using Ax Platform
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from typing import Dict, Tuple
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt

# Random seed for reproducible simulation noise in this example script
RNG = np.random.RandomState(42)

# Domain-specific objective names
STRENGTH_METRIC = "strength"  # MPa
BIODEG_METRIC = "biodegradability_score"  # arbitrary score (0-100 scale in simulator)

# Composition total for monomer fractions
COMPOSITION_TOTAL = 1.0


def simulate_polymer_outcomes_full(
    monomer_1: float,
    monomer_2: float,
    monomer_3: float,
    monomer_4: float,
    monomer_5: float,
    extrusion_rate: float,
    temperature_celsius: float,
) -> Dict[str, float]:
    """
    Simulated evaluation of polymer strength and biodegradability.

    Replace this with your actual lab measurement, instrument integration, or
    high-fidelity simulator call. This function should return a dict mapping
    metric names to measured values (mean). If you know measurement SEM, you
    can pass tuples (mean, SEM) instead when calling `complete_trial`.

    Inputs:
      - monomer_i: fraction of monomer i (must sum to 1)
      - extrusion_rate: mm/s
      - temperature_celsius: °C

    Outputs:
      - "strength": MPa (maximize, aim >= 70 MPa)
      - "biodegradability_score": 0-100 (maximize)
    """
    # Safety: clip small numerical drift and enforce valid composition
    fractions = np.array(
        [monomer_1, monomer_2, monomer_3, monomer_4, monomer_5], dtype=float
    )
    # Normalize tiny deviations if any
    total = fractions.sum()
    if not np.isclose(total, COMPOSITION_TOTAL, atol=1e-6):
        fractions = fractions / total
        monomer_1, monomer_2, monomer_3, monomer_4, monomer_5 = fractions.tolist()

    # Process parameters
    r = extrusion_rate  # mm/s
    T = temperature_celsius  # °C

    # Strength model (synthetic but plausible)
    # Base contributions from monomers (MPa potential contribution)
    s_vec = np.array([95.0, 78.0, 90.0, 82.0, 60.0])
    strength_base = float(np.dot(s_vec, fractions))

    # Process optima
    r_opt = 0.055  # mm/s
    T_opt = 172.0  # °C

    # Quadratic penalties for deviating from optima
    # Scales chosen to keep strengths in a realistic range
    strength_penalty_r = 150.0 * (r - r_opt) ** 2
    strength_penalty_T = 0.08 * (T - T_opt) ** 2

    # Positive synergy between certain monomers (e.g., 1 and 3)
    synergy = 20.0 * (monomer_1 * monomer_3)

    # Slight detriment from too much monomer_5 on strength
    plasticizer_penalty = 25.0 * monomer_5

    strength = strength_base + synergy - strength_penalty_r - strength_penalty_T - plasticizer_penalty

    # Biodegradability model (0-100)
    # High biodegradability for monomer_5 and moderately for monomer_2, monomer_4
    b_vec = np.array([30.0, 55.0, 35.0, 45.0, 85.0])
    biodeg_base = float(np.dot(b_vec, fractions))

    # Processing effects:
    # - Slightly higher biodegradability at higher T (accelerates hydrolysable linkages formation),
    #   but too high can lead to over-crosslinking; model with a broad quadratic around 175°C.
    # - Slower extrusion (lower r) might increase porosity, aiding biodegradability.
    biodeg_T_peak = 175.0
    biodeg_T_effect = -0.03 * (T - biodeg_T_peak) ** 2 + 5.0  # max +5 around 175°C
    biodeg_r_effect = 12.0 * (0.10 - r)  # favors lower r, bounded by range 0.01-0.1

    biodegradability = biodeg_base + biodeg_T_effect + biodeg_r_effect

    # Add realistic measurement noise (if you know SEMs, return tuples instead)
    strength += RNG.normal(0.0, 1.0)
    biodegradability += RNG.normal(0.0, 2.0)

    # Clip to plausible ranges
    strength = float(np.clip(strength, 0.0, 200.0))
    biodegradability = float(np.clip(biodegradability, 0.0, 100.0))

    return {STRENGTH_METRIC: strength, BIODEG_METRIC: biodegradability}


def evaluate_formulation_from_search_params(params: Dict[str, float]) -> Dict[str, float]:
    """
    Evaluate given search parameters: expects monomer_1..monomer_4, extrusion_rate, temperature_celsius.
    Computes monomer_5 via compositional constraint: monomer_5 = 1 - sum(m1..m4).
    """
    m1 = float(params["monomer_1"])
    m2 = float(params["monomer_2"])
    m3 = float(params["monomer_3"])
    m4 = float(params["monomer_4"])
    m5 = COMPOSITION_TOTAL - (m1 + m2 + m3 + m4)

    # Guard against numerical issues; if violated due to rounding, clip to [0,1]
    if m5 < 0.0:
        m5 = 0.0
    if m5 > 1.0:
        m5 = 1.0

    return simulate_polymer_outcomes_full(
        monomer_1=m1,
        monomer_2=m2,
        monomer_3=m3,
        monomer_4=m4,
        monomer_5=m5,
        extrusion_rate=float(params["extrusion_rate"]),
        temperature_celsius=float(params["temperature_celsius"]),
    )


# Historical training data (replace values with your real measurements if available).
# These are example batches satisfying the composition constraint.
historical_df = pd.DataFrame(
    [
        {
            "monomer_1": 0.20,
            "monomer_2": 0.20,
            "monomer_3": 0.20,
            "monomer_4": 0.20,
            "monomer_5": 0.20,
            "extrusion_rate": 0.055,
            "temperature_celsius": 172.0,
        },
        {
            "monomer_1": 0.40,
            "monomer_2": 0.10,
            "monomer_3": 0.30,
            "monomer_4": 0.10,
            "monomer_5": 0.10,
            "extrusion_rate": 0.040,
            "temperature_celsius": 160.0,
        },
        {
            "monomer_1": 0.10,
            "monomer_2": 0.15,
            "monomer_3": 0.25,
            "monomer_4": 0.20,
            "monomer_5": 0.30,
            "extrusion_rate": 0.070,
            "temperature_celsius": 180.0,
        },
    ]
)

# Compute historical objective values (in real use, replace with your measured results)
historical_results = [
    simulate_polymer_outcomes_full(
        row["monomer_1"],
        row["monomer_2"],
        row["monomer_3"],
        row["monomer_4"],
        row["monomer_5"],
        row["extrusion_rate"],
        row["temperature_celsius"],
    )
    for _, row in historical_df.iterrows()
]


# Experiment configuration
TOTAL_TRIAL_BUDGET = 35
BATCH_SIZE = 1  # Sequential
USE_NOISE_MODEL = True  # We simulate noise in the outcomes

ax_client = AxClient(random_seed=0)

# Create experiment with multi-objective optimization.
# Objectives: maximize strength (threshold at 70 MPa) and maximize biodegradability_score.
ax_client.create_experiment(
    name="polymer_formulation_moo",
    parameters=[
        # Reparameterize composition: search over first 4 monomers; compute the 5th
        {"name": "monomer_1", "type": "range", "bounds": [0.0, 1.0]},
        {"name": "monomer_2", "type": "range", "bounds": [0.0, 1.0]},
        {"name": "monomer_3", "type": "range", "bounds": [0.0, 1.0]},
        {"name": "monomer_4", "type": "range", "bounds": [0.0, 1.0]},
        {"name": "extrusion_rate", "type": "range", "bounds": [0.01, 0.10]},  # mm/s
        {"name": "temperature_celsius", "type": "range", "bounds": [120.0, 200.0]},  # °C
    ],
    objectives={
        STRENGTH_METRIC: ObjectiveProperties(minimize=False, threshold=70.0),
        # Threshold for biodegradability is optional; Ax will infer if omitted
        BIODEG_METRIC: ObjectiveProperties(minimize=False),
    },
    # Enforce composition: monomer_1 + ... + monomer_4 <= 1.0, with monomer_5 = 1 - sum(...)
    parameter_constraints=[
        "monomer_1 + monomer_2 + monomer_3 + monomer_4 <= 1.0",
    ],
)

# Attach historical data to AxClient
for i in range(len(historical_df)):
    # Parameterization for search space excludes monomer_5 (computed internally)
    parameterization = {
        "monomer_1": float(historical_df.iloc[i]["monomer_1"]),
        "monomer_2": float(historical_df.iloc[i]["monomer_2"]),
        "monomer_3": float(historical_df.iloc[i]["monomer_3"]),
        "monomer_4": float(historical_df.iloc[i]["monomer_4"]),
        "extrusion_rate": float(historical_df.iloc[i]["extrusion_rate"]),
        "temperature_celsius": float(historical_df.iloc[i]["temperature_celsius"]),
    }

    ax_client.attach_trial(parameterization)
    # Use historical results (if you have real measurements, use those here)
    ax_client.complete_trial(trial_index=i, raw_data=historical_results[i])

# Run remaining trials up to the budget
n_train = len(historical_df)
n_remaining = max(0, TOTAL_TRIAL_BUDGET - n_train)

for _ in range(n_remaining):
    parameters, trial_index = ax_client.get_next_trial()
    # Evaluate the candidate (replace with your lab execution + data collection)
    results = evaluate_formulation_from_search_params(parameters)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)

# Retrieve Pareto-optimal points based on observed data
pareto_dict = ax_client.get_pareto_optimal_parameters(use_model_predictions=False)

# Plot observed data and the empirical Pareto front
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
# Scatter of all observed points
ax.scatter(
    df[objectives[0]], df[objectives[1]], fc="None", ec="k", label="Observed"
)

# Build a DataFrame of Pareto points for plotting the frontier
pareto_data = []
for arm_name, (params, metrics) in pareto_dict.items():
    pareto_data.append({objectives[0]: metrics[0][0], objectives[1]: metrics[1][0]})
if pareto_data:
    pareto_df = pd.DataFrame(pareto_data).sort_values(objectives[0])
    ax.plot(
        pareto_df[objectives[0]],
        pareto_df[objectives[1]],
        color="#0033FF",
        lw=2,
        label="Pareto Front",
    )

ax.set_xlabel(f"{STRENGTH_METRIC} (MPa)")
ax.set_ylabel(f"{BIODEG_METRIC} (score)")
ax.legend()
plt.tight_layout()
plt.show()

# Print Pareto-optimal parameterizations (including computed monomer_5)
print("Pareto-optimal candidates (observed):")
for arm_name, (params, metrics) in pareto_dict.items():
    m1 = params["monomer_1"]
    m2 = params["monomer_2"]
    m3 = params["monomer_3"]
    m4 = params["monomer_4"]
    m5 = COMPOSITION_TOTAL - (m1 + m2 + m3 + m4)
    strength_val = metrics[0][0]
    biodeg_val = metrics[1][0]
    print(
        f"- {arm_name}: "
        f"m1={m1:.3f}, m2={m2:.3f}, m3={m3:.3f}, m4={m4:.3f}, m5={m5:.3f}, "
        f"rate={params['extrusion_rate']:.3f} mm/s, T={params['temperature_celsius']:.1f} °C | "
        f"strength={strength_val:.2f} MPa, biodegradability={biodeg_val:.2f}"
    )