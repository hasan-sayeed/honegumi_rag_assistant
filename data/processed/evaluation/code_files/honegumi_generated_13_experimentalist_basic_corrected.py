# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


obj1_name = "branin"


def branin(x1, x2):
    y = float(
        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)
        + 10
    )

    return y


ax_client = AxClient()

ax_client.create_experiment(
    parameters=[
        {"name": "x1", "type": "range", "bounds": [-5.0, 10.0]},
        {"name": "x2", "type": "range", "bounds": [0.0, 10.0]},
    ],
    objectives={
        obj1_name: ObjectiveProperties(minimize=True),
    },
)


for i in range(19):

    parameterization, trial_index = ax_client.get_next_trial()

    # extract parameters
    x1 = parameterization["x1"]
    x2 = parameterization["x2"]

    results = branin(x1, x2)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)

best_parameters, metrics = ax_client.get_best_parameters()


# Plot results
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
ax.scatter(df.index, df[objectives], ec="k", fc="none", label="Observed")
ax.plot(
    df.index,
    np.minimum.accumulate(df[objectives]),
    color="#0033FF",
    lw=2,
    label="Best to Trial",
)
ax.set_xlabel("Trial Number")
ax.set_ylabel(objectives[0])

ax.legend()
plt.show()

# Benchmark acquisition functions (EI, UCB, Thompson Sampling) on the Branin–Hoo function
# %pip install ax-platform==0.4.3 matplotlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from typing import Dict, List, Tuple

from ax.service.ax_client import AxClient, ObjectiveProperties
from ax.modelbridge.generation_strategy import GenerationStrategy, GenerationStep
from ax.modelbridge.registry import Models
from ax.models.torch.botorch_modular.surrogate import Surrogate

from botorch.models.gp_regression import SingleTaskGP
from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement, qThompsonSampling
from botorch.acquisition.analytic import UpperConfidenceBound


# --------------------------------------------------------------------------------------
# Domain-specific problem: Branin–Hoo synthetic function minimization
# Search space:
#   x_coordinate in [-5, 10]
#   y_coordinate in [0, 15]
# Objective:
#   Minimize branin_hoo_value
# Benchmark setup:
#   - Acquisition functions: EI, UCB, Thompson Sampling
#   - 20 independent runs per acquisition function
#   - 30 total evaluations per run (with 5 Sobol initial evaluations)
# --------------------------------------------------------------------------------------

OBJECTIVE_NAME = "branin_hoo_value"
X_PARAM = "x_coordinate"
Y_PARAM = "y_coordinate"

X_BOUNDS = [-5.0, 10.0]
Y_BOUNDS = [0.0, 15.0]

TOTAL_EVALUATIONS_PER_RUN = 30
NUM_SOBOL_INIT = 5  # Initialization trials via Sobol
NUM_BO_EVALS = TOTAL_EVALUATIONS_PER_RUN - NUM_SOBOL_INIT
NUM_RUNS_PER_ACQ = 20

# Known global minimum of Branin-Hoo is approximately 0.397887
BRANIN_GLOBAL_MIN = 0.397887


def compute_branin_hoo(x: float, y: float) -> float:
    """Compute the Branin–Hoo function value at (x, y)."""
    # Ensure inputs within bounds (useful if any tiny numerical drift occurs)
    x = float(np.clip(x, X_BOUNDS[0], X_BOUNDS[1]))
    y = float(np.clip(y, Y_BOUNDS[0], Y_BOUNDS[1]))

    a = 1.0
    b = 5.1 / (4.0 * np.pi**2)
    c = 5.0 / np.pi
    r = 6.0
    s = 10.0
    t = 1.0 / (8.0 * np.pi)

    return a * (y - b * x**2 + c * x - r) ** 2 + s * (1 - t) * np.cos(x) + s


def evaluate_branin_experiment(parameters: Dict[str, float]) -> float:
    """Evaluate and return the Branin–Hoo objective value for AxClient.
    Returns a single float to enable Ax to infer observation noise (noise model = True)."""
    x = parameters[X_PARAM]
    y = parameters[Y_PARAM]
    return compute_branin_hoo(x, y)


def make_generation_strategy_for_acq(
    acq_name: str,
    seed: int,
    ucb_beta: float = 2.0,
) -> GenerationStrategy:
    """Build a GenerationStrategy for a given acquisition function."""
    # Common Sobol initialization step
    sobol_step = GenerationStep(
        model=Models.SOBOL,
        num_trials=NUM_SOBOL_INIT,
        min_trials_observed=NUM_SOBOL_INIT,
        max_parallelism=NUM_SOBOL_INIT,
        model_kwargs={"seed": seed},
    )

    # Configure BoTorch modular model with chosen acquisition function
    if acq_name == "EI":
        bo_step = GenerationStep(
            model=Models.BOTORCH_MODULAR,
            num_trials=NUM_BO_EVALS,
            max_parallelism=1,  # enforce sequential BO for fairness
            model_kwargs={
                "surrogate": Surrogate(SingleTaskGP),
                "botorch_acqf_class": qNoisyExpectedImprovement,
            },
        )
    elif acq_name == "UCB":
        bo_step = GenerationStep(
            model=Models.BOTORCH_MODULAR,
            num_trials=NUM_BO_EVALS,
            max_parallelism=1,
            model_kwargs={
                "surrogate": Surrogate(SingleTaskGP),
                "botorch_acqf_class": UpperConfidenceBound,
                # Pass beta to the acquisition. Larger beta -> more exploration.
                "acquisition_options": {"beta": float(ucb_beta)},
            },
        )
    elif acq_name == "TS":
        bo_step = GenerationStep(
            model=Models.BOTORCH_MODULAR,
            num_trials=NUM_BO_EVALS,
            max_parallelism=1,
            model_kwargs={
                "surrogate": Surrogate(SingleTaskGP),
                "botorch_acqf_class": qThompsonSampling,
            },
        )
    else:
        raise ValueError(f"Unknown acquisition function name: {acq_name}")

    return GenerationStrategy(steps=[sobol_step, bo_step])


def run_single_benchmark_run(
    acq_name: str,
    run_seed: int,
    ucb_beta: float = 2.0,
) -> Tuple[List[float], AxClient]:
    """Execute one optimization run for the specified acquisition function.

    Returns:
        best_so_far_trace: list of best objective values after each evaluation (length = TOTAL_EVALUATIONS_PER_RUN)
        ax_client: the AxClient used (for optional further inspection)
    """
    np.random.seed(run_seed)

    generation_strategy = make_generation_strategy_for_acq(
        acq_name=acq_name,
        seed=run_seed,
        ucb_beta=ucb_beta,
    )

    ax_client = AxClient(generation_strategy=generation_strategy)

    ax_client.create_experiment(
        name=f"branin_benchmark_{acq_name}_seed_{run_seed}",
        parameters=[
            {"name": X_PARAM, "type": "range", "bounds": X_BOUNDS, "value_type": "float"},
            {"name": Y_PARAM, "type": "range", "bounds": Y_BOUNDS, "value_type": "float"},
        ],
        objectives={OBJECTIVE_NAME: ObjectiveProperties(minimize=True)},
    )

    best_so_far_trace: List[float] = []
    current_best = float("inf")

    for _ in range(TOTAL_EVALUATIONS_PER_RUN):
        parameters, trial_index = ax_client.get_next_trial()
        objective_value = evaluate_branin_experiment(parameters)
        # Return a single mean value (float) so Ax will infer noise (noise model = True)
        ax_client.complete_trial(trial_index=trial_index, raw_data=objective_value)

        current_best = min(current_best, objective_value)
        best_so_far_trace.append(current_best)

    return best_so_far_trace, ax_client


def benchmark_acquisitions(
    acquisition_set: List[str],
    runs_per_acq: int,
    ucb_beta: float = 2.0,
    base_seed: int = 12345,
) -> Dict[str, np.ndarray]:
    """Run multiple optimization runs for each acquisition function and collect best-so-far traces.

    Returns:
        results: mapping from acquisition name to array of shape (runs_per_acq, TOTAL_EVALUATIONS_PER_RUN)
    """
    results: Dict[str, np.ndarray] = {}
    for acq_idx, acq_name in enumerate(acquisition_set):
        traces = []
        for r in range(runs_per_acq):
            run_seed = base_seed + acq_idx * 1000 + r
            trace, _ = run_single_benchmark_run(acq_name=acq_name, run_seed=run_seed, ucb_beta=ucb_beta)
            traces.append(trace)
        results[acq_name] = np.array(traces)
    return results


def plot_optimization_traces(results: Dict[str, np.ndarray], ucb_beta: float) -> None:
    """Plot mean ± std best-so-far optimization traces for each acquisition function."""
    plt.figure(figsize=(8, 5), dpi=150)
    eval_indices = np.arange(1, TOTAL_EVALUATIONS_PER_RUN + 1)

    color_map = {
        "EI": "#0033FF",
        "UCB": "#FF7F0E",
        "TS": "#2CA02C",
    }

    for acq_name, traces in results.items():
        means = traces.mean(axis=0)
        stds = traces.std(axis=0)
        label = acq_name if acq_name != "UCB" else f"UCB (beta={ucb_beta})"
        plt.plot(eval_indices, means, label=label, color=color_map.get(acq_name, None), lw=2)
        plt.fill_between(eval_indices, means - stds, means + stds, color=color_map.get(acq_name, None), alpha=0.15)

    # Plot known global minimum for reference
    plt.axhline(BRANIN_GLOBAL_MIN, color="k", ls="--", lw=1.5, label="Global minimum (~0.398)")

    plt.xlabel("Evaluation number")
    plt.ylabel("Best Branin–Hoo value found")
    plt.title("Bayesian Optimization on Branin–Hoo: Acquisition Function Benchmark")
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()


def main():
    acquisition_functions = ["EI", "UCB", "TS"]
    ucb_beta = 2.0

    results = benchmark_acquisitions(
        acquisition_set=acquisition_functions,
        runs_per_acq=NUM_RUNS_PER_ACQ,
        ucb_beta=ucb_beta,
        base_seed=202311,
    )

    # Print final average best values for a quick textual summary
    print("Final best values after 30 evaluations (mean ± std) over 20 runs:")
    for acq_name in acquisition_functions:
        traces = results[acq_name]
        final_vals = traces[:, -1]
        print(f"  {acq_name:>2}: {final_vals.mean():.4f} ± {final_vals.std():.4f}")

    plot_optimization_traces(results, ucb_beta=ucb_beta)


if __name__ == "__main__":
    main()