# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


obj1_name = "branin"


def branin(x1, x2):
    y = float(
        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)
        + 10
    )

    return y


ax_client = AxClient()

ax_client.create_experiment(
    parameters=[
        {"name": "x1", "type": "range", "bounds": [-5.0, 10.0]},
        {"name": "x2", "type": "range", "bounds": [0.0, 10.0]},
    ],
    objectives={
        obj1_name: ObjectiveProperties(minimize=True),
    },
)


for i in range(19):

    parameterization, trial_index = ax_client.get_next_trial()

    # extract parameters
    x1 = parameterization["x1"]
    x2 = parameterization["x2"]

    results = branin(x1, x2)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)

best_parameters, metrics = ax_client.get_best_parameters()


# Plot results
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
ax.scatter(df.index, df[objectives], ec="k", fc="none", label="Observed")
ax.plot(
    df.index,
    np.minimum.accumulate(df[objectives]),
    color="#0033FF",
    lw=2,
    label="Best to Trial",
)
ax.set_xlabel("Trial Number")
ax.set_ylabel(objectives[0])

ax.legend()
plt.show()

# Acquisition function benchmark on Branin-Hoo (minimize)
# Compares EI, UCB, and Thompson Sampling using Ax + BoTorch
# %pip install ax-platform==0.4.3 matplotlib numpy pandas
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from typing import Dict, List, Tuple

from ax.service.ax_client import AxClient, ObjectiveProperties
from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
from ax.modelbridge.registry import Models

from botorch.acquisition.analytic import ExpectedImprovement
from botorch.acquisition.monte_carlo import qExpectedImprovement, qThompsonSampling, qUpperConfidenceBound


# ---- Branin-Hoo objective (to minimize) ----
def branin_hoo_function(x1: float, x2: float) -> float:
    # Standard Branin function with x1 in [-5, 10], x2 in [0, 15]
    a = 1.0
    b = 5.1 / (4.0 * math.pi**2)
    c = 5.0 / math.pi
    r = 6.0
    s = 10.0
    t = 1.0 / (8.0 * math.pi)
    return float(a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * math.cos(x1) + s)


BRANIN_GLOBAL_MIN = 0.39788735772973816  # Known global minimum value


# ---- Generation strategy builders for each acquisition method ----
def build_generation_strategy(acqf_method: str, seed: int, sobol_init: int, total_evals: int) -> GenerationStrategy:
    if acqf_method.upper() == "EI":
        # Use qExpectedImprovement for deterministic settings
        acqf_class = qExpectedImprovement
        acqf_options = {}  # No special options required
    elif acqf_method.upper() == "UCB":
        acqf_class = qUpperConfidenceBound
        acqf_options = {"beta": 2.0}  # Typical beta, controls exploration-exploitation
    elif acqf_method.upper() == "TS":
        acqf_class = qThompsonSampling
        acqf_options = {}  # Defaults create a posterior sampler internally
    else:
        raise ValueError(f"Unknown acquisition method: {acqf_method}")

    return GenerationStrategy(
        steps=[
            GenerationStep(
                model=Models.SOBOL,
                num_trials=sobol_init,
                min_trials_observed=sobol_init,
                max_parallelism=1,
                model_kwargs={"seed": seed},
            ),
            GenerationStep(
                model=Models.BOTORCH_MODULAR,
                num_trials=total_evals - sobol_init,
                max_parallelism=1,
                # BoTorchModel kwargs
                model_kwargs={
                    "botorch_acqf_class": acqf_class,
                    "acquisition_options": acqf_options,
                },
            ),
        ]
    )


# ---- Single optimization run with AxClient ----
def run_single_branin_optimization(
    acqf_method: str,
    run_id: int,
    total_evals: int,
    sobol_init: int,
    seed_base: int = 12345,
) -> Tuple[np.ndarray, List[float]]:
    # Build generation strategy for this run/method
    gs_seed = seed_base + run_id
    gs = build_generation_strategy(acqf_method=acqf_method, seed=gs_seed, sobol_init=sobol_init, total_evals=total_evals)

    ax_client = AxClient(generation_strategy=gs, verbose_logging=False)
    ax_client.create_experiment(
        name=f"branin_benchmark_{acqf_method}_run_{run_id}",
        parameters=[
            {"name": "branin_x1", "type": "range", "bounds": [-5.0, 10.0], "value_type": "float"},
            {"name": "branin_x2", "type": "range", "bounds": [0.0, 15.0], "value_type": "float"},
        ],
        objectives={"branin_hoo_value": ObjectiveProperties(minimize=True)},
    )

    observed_values: List[float] = []
    best_so_far: List[float] = []

    for _ in range(total_evals):
        params, trial_index = ax_client.get_next_trial()
        x1 = float(params["branin_x1"])
        x2 = float(params["branin_x2"])
        y = branin_hoo_function(x1, x2)

        # Deterministic evaluation: SEM = 0.0
        ax_client.complete_trial(trial_index=trial_index, raw_data={"branin_hoo_value": (y, 0.0)})

        observed_values.append(y)
        current_best = np.min(observed_values)
        best_so_far.append(current_best)

    return np.asarray(best_so_far, dtype=float), observed_values


# ---- Benchmark runner ----
def run_benchmark(
    acquisition_methods: List[str],
    trials_per_method: int,
    evaluations_per_trial: int,
    sobol_init: int,
    seed_base: int = 20241031,
) -> Dict[str, Dict[str, np.ndarray]]:
    results: Dict[str, Dict[str, np.ndarray]] = {}
    for method in acquisition_methods:
        all_runs_best = np.zeros((trials_per_method, evaluations_per_trial), dtype=float)
        for run_idx in range(trials_per_method):
            best_curve, _ = run_single_branin_optimization(
                acqf_method=method,
                run_id=run_idx,
                total_evals=evaluations_per_trial,
                sobol_init=sobol_init,
                seed_base=seed_base,
            )
            all_runs_best[run_idx, :] = best_curve
        results[method] = {"best_so_far": all_runs_best}
    return results


# ---- Statistical analysis and visualization ----
def analyze_and_plot(
    results: Dict[str, Dict[str, np.ndarray]],
    evaluations_per_trial: int,
    title_suffix: str = "",
    show_plots: bool = True,
):
    eval_axis = np.arange(1, evaluations_per_trial + 1, dtype=int)

    plt.figure(figsize=(8, 5), dpi=150)
    for method, data in results.items():
        best_matrix = data["best_so_far"]  # shape: [n_runs, n_evals]
        mean_curve = best_matrix.mean(axis=0)
        std_curve = best_matrix.std(axis=0, ddof=1)
        # 95% CI assuming approx normal across runs
        n_runs = best_matrix.shape[0]
        ci = 1.96 * (std_curve / np.sqrt(max(n_runs, 1)))

        plt.plot(eval_axis, mean_curve, lw=2, label=f"{method} mean")
        plt.fill_between(eval_axis, mean_curve - ci, mean_curve + ci, alpha=0.2)

    plt.xlabel("Function evaluations")
    plt.ylabel("Best-so-far Branin value (lower is better)")
    plt.title(f"Branin-Hoo Optimization Convergence{f' - {title_suffix}' if title_suffix else ''}")
    plt.legend()
    plt.grid(alpha=0.3)

    # Final performance boxplot (in terms of value and regret)
    final_values = []
    labels = []
    for method, data in results.items():
        finals = data["best_so_far"][:, -1]
        final_values.append(finals)
        labels.append(method)

    plt.figure(figsize=(7, 4), dpi=150)
    plt.boxplot(final_values, labels=labels, showmeans=True)
    plt.ylabel("Final best Branin value (after budget)")
    plt.title("Final Performance Distribution Across Runs")

    # Compute and print summary statistics
    summary_rows = []
    for method, data in results.items():
        finals = data["best_so_far"][:, -1]
        regrets = finals - BRANIN_GLOBAL_MIN
        summary_rows.append(
            {
                "method": method,
                "mean_final_value": finals.mean(),
                "median_final_value": np.median(finals),
                "std_final_value": finals.std(ddof=1),
                "mean_final_regret": regrets.mean(),
                "median_final_regret": np.median(regrets),
                "std_final_regret": regrets.std(ddof=1),
            }
        )
    summary_df = pd.DataFrame(summary_rows).set_index("method").sort_index()
    print("\n==== Benchmark Summary (Final Best Values and Regrets) ====\n")
    print(summary_df.to_string(float_format=lambda v: f"{v:0.6f}"))

    if show_plots:
        plt.show()


if __name__ == "__main__":
    # Configuration per problem description:
    # - 3 acquisition strategies: EI, UCB, Thompson Sampling
    # - 20 trials (independent runs) per method
    # - 30 evaluations per trial
    # - Deterministic noiseless Branin, domain: x1 ∈ [-5, 10], x2 ∈ [0, 15]
    acquisition_methods = ["EI", "UCB", "TS"]
    trials_per_method = 20
    evaluations_per_trial = 30
    sobol_init = 5  # Number of initial Sobol points before BO starts

    results = run_benchmark(
        acquisition_methods=acquisition_methods,
        trials_per_method=trials_per_method,
        evaluations_per_trial=evaluations_per_trial,
        sobol_init=sobol_init,
        seed_base=20251031,
    )

    analyze_and_plot(
        results=results,
        evaluations_per_trial=evaluations_per_trial,
        title_suffix=f"{trials_per_method} runs, {evaluations_per_trial} evals/run",
        show_plots=True,
    )